{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_web(search_topic, location):\n",
    "    MY_ENV_VAR = os.getenv('SERP_API_AUTH')\n",
    "    api_key = MY_ENV_VAR\n",
    "    api_endpoint = 'https://serpapi.com/search'\n",
    "\n",
    "    params = {\n",
    "        'q': search_topic,\n",
    "        'location': location,\n",
    "        'api_key': api_key\n",
    "    }\n",
    "\n",
    "    response = requests.get(api_endpoint, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    websites = [result['link'] for result in data['organic_results']]\n",
    "    website_names = [result['title'] for result in data['organic_results']]\n",
    "\n",
    "    for website_name in website_names:\n",
    "        print(website_name)\n",
    "\n",
    "    return websites\n",
    "\n",
    "# # Example usage\n",
    "# search_web('best bagels in Seattle', 'Seattle, WA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hire the best React.js developers\n",
      "11 Best Freelance React.js Developers [Hire in 48 Hours]\n",
      "11 Best Freelance Full-Stack Developers [Hire in 48 Hours]\n",
      "Hire the best React Native developers\n",
      "Hire Python and React.js Developers\n",
      "Remote Python/React developer jobs\n",
      "Hire React Native Developers With Lemon.io\n",
      "Who wants to be hired (May 2023) Â· vercel next.js\n",
      "Developers that have started their own business : r/webdev\n",
      "I think I haven't made any progress even after investing ...\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# goal\n",
    "goal = \"I need few developers for my new startup, I am looking for people with 2 years of experience in python and react.\"\n",
    "location = \"Bengaluru, Karnataka, India\"\n",
    "\n",
    "# fetch the sites\n",
    "websites = search_web(goal, location)\n",
    "\n",
    "flag1 = time.time()\n",
    "print(\"Time taken to fetch the sites: \", flag1 - start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.upwork.com/hire/react-js-developers/',\n",
       " 'https://www.toptal.com/react',\n",
       " 'https://www.toptal.com/full-stack',\n",
       " 'https://www.upwork.com/hire/react-native-developers/',\n",
       " 'https://www.voypost.com/hire-python-and-react-js-developers',\n",
       " 'https://www.turing.com/jobs/remote-python-react-developer',\n",
       " 'https://lemon.io/hire-react-native-developers/',\n",
       " 'https://github.com/vercel/next.js/discussions/49472',\n",
       " 'https://www.reddit.com/r/webdev/comments/12hz450/developers_that_have_started_their_own_business/',\n",
       " 'https://forum.freecodecamp.org/t/i-think-i-havent-made-any-progress-even-after-investing-the-time-give-some-advice/438513']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_html_junk(data):\n",
    "    # remove all class names\n",
    "    data = re.sub(r'(?is)class=\"[^\"]*\"', '', data)\n",
    "    data = re.sub(r'(?is)className=\"[^\"]*\"', '', data)\n",
    "\n",
    "    # remove all inline styles\n",
    "    data = re.sub(r'(?is)style=\"[^\"]*\"', '', data)\n",
    "\n",
    "    # remove all aria-label\n",
    "    data = re.sub(r'(?is)aria-label=\"[^\"]*\"', '', data)\n",
    "\n",
    "    # remove all comments\n",
    "    data = re.sub(r'(?s)<!--(.*?)-->[\\n]?', '', data)\n",
    "\n",
    "    # remove all ids\n",
    "    data = re.sub(r'(?is)id=\"[^\"]*\"', '', data)\n",
    "\n",
    "    # remove all newlines, long space and tabs\n",
    "    data = re.sub(r'[\\n\\t]', ' ', data)\n",
    "    data = re.sub(r' +', ' ', data)\n",
    "\n",
    "    # remove all non-imp tags both open and close\n",
    "    remove_tags = [\"span\", \"div\", \"strong\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]   \n",
    "    for tag in remove_tags:\n",
    "        data = re.sub(r'(?is)<{}[^>]*>'.format(tag), '', data)\n",
    "        data = re.sub(r'(?is)</{}[^>]*>'.format(tag), '', data) \n",
    "\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m website_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://python.langchain.com/docs/use_cases/web_scraping\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m data \u001b[39m=\u001b[39m scrape_data(website_url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb#W5sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mprint\u001b[39m(data)\n",
      "\u001b[1;32m/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb#W5sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         data \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mbody\u001b[39m.\u001b[39mprettify()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb#W5sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39m# data = remove_html_junk(soup.main.prettify())\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     data \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mhandle(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anshuman/Anshu/Projects/PotentGPT/testings/potentgpt.ipynb#W5sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Anshu/Projects/PotentGPT/testings/html2text.py:243\u001b[0m, in \u001b[0;36mHTML2Text.handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandle\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed(data)\n\u001b[1;32m    244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptwrap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose())\n",
      "File \u001b[0;32m~/Anshu/Projects/PotentGPT/testings/html2text.py:240\u001b[0m, in \u001b[0;36mHTML2Text.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m    239\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m'\u001b[39m\u001b[39m + \u001b[39m\u001b[39m'\u001b[39m\u001b[39mscript>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m</ignore>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 240\u001b[0m     HTMLParser\u001b[39m.\u001b[39mHTMLParser\u001b[39m.\u001b[39mfeed(\u001b[39mself\u001b[39m, data)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-env/lib/python3.11/html/parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[0;32m--> 110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoahead(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-env/lib/python3.11/html/parser.py:162\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m j:\n\u001b[1;32m    161\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_charrefs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcdata_elem:\n\u001b[0;32m--> 162\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_data(unescape(rawdata[i:j]))\n\u001b[1;32m    163\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_data(rawdata[i:j])\n",
      "File \u001b[0;32m~/Anshu/Projects/PotentGPT/testings/html2text.py:668\u001b[0m, in \u001b[0;36mHTML2Text.handle_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcode \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre:\n\u001b[1;32m    667\u001b[0m     data \u001b[39m=\u001b[39m escape_md_section(data, snob\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mescape_snob)\n\u001b[0;32m--> 668\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mo(data, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Anshu/Projects/PotentGPT/testings/html2text.py:598\u001b[0m, in \u001b[0;36mHTML2Text.o\u001b[0;34m(self, data, puredata, force)\u001b[0m\n\u001b[1;32m    596\u001b[0m     bq \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m \u001b[39m#else: list content is already partially indented\u001b[39;00m\n\u001b[0;32m--> 598\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m xrange(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlist)):\n\u001b[1;32m    599\u001b[0m     bq \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mbq)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "h = html2text.HTML2Text()\n",
    "\n",
    "def scrape_data(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        unwanted_tags = [\n",
    "            \"pre\",\n",
    "            \"code\",\n",
    "            \"blockquote\",\n",
    "            \"em\",\n",
    "            \"br\",\n",
    "            \"source\",\n",
    "            \"circle\",\n",
    "            \"svg\",\n",
    "            \"img\",\n",
    "            \"button\",\n",
    "            \"input\",\n",
    "            \"form\",\n",
    "            \"footer\",\n",
    "            \"header\",\n",
    "            \"aside\",\n",
    "            \"nav\",\n",
    "            \"script\",\n",
    "            \"style\",\n",
    "            \"noscript\",\n",
    "            \"iframe\",\n",
    "            \"meta\",\n",
    "            \"head\",\n",
    "        ]\n",
    "        # for tag in unwanted_tags:\n",
    "        #     for match in soup.find_all(tag):\n",
    "        #         match.decompose()\n",
    "        \n",
    "        try:\n",
    "            # data = soup.body.prettify()\n",
    "            data = soup.main.prettify()\n",
    "        except:\n",
    "            data = soup.body.prettify()\n",
    "\n",
    "        # data = remove_html_junk(soup.main.prettify())\n",
    "        data = h.handle(data)\n",
    "        return data\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Error: Unable to fetch data from {url}. Status code: {response.status_code}\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "\n",
    "website_url = \"https://python.langchain.com/docs/use_cases/web_scraping\"\n",
    "data = scrape_data(website_url)\n",
    "len(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langchain.document_transformers import Html2TextTransformer, beautiful_soup_transformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"news_article_title\": {\"type\": \"string\"},\n",
    "        \"news_article_summary\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"news_article_title\", \"news_article_summary\"],\n",
    "}\n",
    "\n",
    "\n",
    "def extract(content: str, schema: dict):\n",
    "    return create_extraction_chain(schema=schema, llm=llm).run(content)\n",
    "\n",
    "\n",
    "def scrape_with_playwright(urls, schema):\n",
    "    loader = AsyncChromiumLoader(urls)\n",
    "    docs = loader.load()\n",
    "    bs_transformer = beautiful_soup_transformer.BeautifulSoupTransformer()\n",
    "    docs_transformed = bs_transformer.transform_documents(\n",
    "        docs, tags_to_extract=[\"span\"]\n",
    "    )\n",
    "    print(\"Extracting content with LLM\")\n",
    "\n",
    "    # Grab the first 1000 tokens of the site\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=1000, chunk_overlap=0\n",
    "    )\n",
    "    splits = splitter.split_documents(docs_transformed)\n",
    "\n",
    "    # Process the first split\n",
    "    extracted_content = extract(schema=schema, content=splits[0].page_content)\n",
    "    pprint.pprint(extracted_content)\n",
    "    return extracted_content\n",
    "\n",
    "\n",
    "urls = [\"https://www.wsj.com\"]\n",
    "extracted_content = scrape_with_playwright(urls, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "\n",
    "# Create an instance of the HTML2Text class\n",
    "h = html2text.HTML2Text()\n",
    "\n",
    "# Use the instance to convert HTML to text\n",
    "html_content = \"<p>Hello, World!</p>\"\n",
    "text_content = h.handle(html_content)\n",
    "\n",
    "print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MY_ENV_VAR = os.getenv('OpenAI_Authorization')\n",
    "client = OpenAI(api_key=MY_ENV_VAR)\n",
    "print(\"client ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant, and your job is to get new ideas and information for your boss.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "EXAMPLE1 = \"I want to hire a chef for this weekend to make South Indian cuisine for 50 people.\"\n",
    "\n",
    "# EXAMPLE1_ANS = \"\"\"\n",
    "# Questions for the vendor or candidate:[\n",
    "# How many years of experience do you have as a chef?,\n",
    "# What is your hourly charge in rupees?,\n",
    "# Do you have expertise in specific cuisines or types of dishes? Will you be able to make South Indian cuisine?,\n",
    "# Are you comfortable with the dynamic work hours and cooking food for more than 50 people?,\n",
    "# How do you interact with customers to understand their preferences and ensure satisfaction?\n",
    "# ]\n",
    "# \"\"\"\n",
    "\n",
    "PROMPT = \"I need a flat in banglore for rent.\"\n",
    "PROMPT2 = \"I need few developers for my new startup, I am looking for people with 2 years of experience in python and react.\"\n",
    "\n",
    "begin = time.time() \n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  n=1,\n",
    "  model = \"gpt-3.5-turbo\",\n",
    "  temperature = 0.2,\n",
    "  max_tokens = 400,\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": EXAMPLE1},\n",
    "    {\"role\": \"assistant\", \"content\": EXAMPLE1_ANS},\n",
    "    {\"role\": \"user\", \"content\": PROMPT2}\n",
    "  ]\n",
    ")\n",
    "\n",
    "end = time.time() \n",
    "print(completion.choices[0].message.content)\n",
    "print(\"\\n\",(end - begin) , \" Seconds\")\n",
    "print(f'Total Tokens:{completion.usage.total_tokens}, Prompt Token {completion.usage.prompt_tokens}, Completion Token {completion.usage.completion_tokens}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
